{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Factorization_Machine.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6nB97KqUFHS"
      },
      "outputs": [],
      "source": [
        "#Libraries and packages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "#import cPickle5 as cPickle\n",
        "from collections import defaultdict\n",
        "import sys, re\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "#Surprise library\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection.validation import cross_validate\n",
        "from surprise.dataset import Dataset\n",
        "from surprise.reader import Reader\n",
        "from surprise import SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise.model_selection import RandomizedSearchCV\n",
        "from collections import defaultdict\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split as sktrain_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from numba import njit\n",
        "from tqdm import trange\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FactorizationMachineClassifier(BaseEstimator, ClassifierMixin):\n",
        "  def __init__(self, n_iter = 10, n_factors = 10, learning_rate = 0.1, reg_coef = 0.01,\n",
        "                 reg_factors = 0.01, random_state = 1234, verbose = False):\n",
        "    self.n_iter = n_iter\n",
        "    self.verbose = verbose\n",
        "    self.reg_coef = reg_coef\n",
        "    self.n_factors = n_factors\n",
        "    self.reg_factors = reg_factors\n",
        "    self.random_state = random_state\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def fit(self, X, y):\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    self.coef_ = np.zeros(n_features)\n",
        "    self.intercept_ = 0.0\n",
        "\n",
        "    np.random.seed(self.random_state)\n",
        "    self.feature_factors_ = np.random.normal(scale = 1 / np.sqrt(self.n_factors), size = (self.n_factors, n_features))\n",
        "        \n",
        "    y = y.copy().astype(np.int32)\n",
        "\n",
        "    loop = range(self.n_iter)\n",
        "    if self.verbose:\n",
        "        loop = trange(self.n_iter)\n",
        "\n",
        "    self.history_ = []\n",
        "    for _ in loop:\n",
        "        loss = _sgd_update(X.data, X.indptr, X.indices,y, n_samples, n_features,self.intercept_, self.coef_,self.feature_factors_, self.n_factors,self.learning_rate, self.reg_coef, self.reg_factors)\n",
        "        self.history_.append(loss)\n",
        "\n",
        "    return self\n",
        "\n",
        "  def predict_proba(self, X):\n",
        "    pred = self._predict(X)\n",
        "    return pred\n",
        "\n",
        "  def _predict(self, X):\n",
        "    linear_output = X * self.coef_\n",
        "    v = self.feature_factors_.T\n",
        "    term = (X * v) ** 2 - (X.power(2) * (v ** 2))\n",
        "    factor_output = 0.5 * np.sum(term, axis = 1)\n",
        "    return self.intercept_ + linear_output + factor_output\n",
        "\n",
        "  def predict(self, X):\n",
        "    pred_proba = self.predict_proba(X)\n",
        "    return pred_proba.astype(np.int)\n",
        "\n",
        "\n",
        "@njit\n",
        "def _sgd_update(data, indptr, indices, y, n_samples, n_features,\n",
        "                w0, w, v, n_factors, learning_rate, reg_w, reg_v):\n",
        "    \"\"\"\n",
        "    Compute the loss of the current iteration and update\n",
        "    gradients accordingly.\n",
        "    \"\"\"\n",
        "  loss = 0.0\n",
        "  for i in range(n_samples):\n",
        "    pred, summed = _predict_instance(data, indptr, indices, w0, w, v, n_factors, i)\n",
        "        \n",
        "    loss += _log_loss(pred, y[i])\n",
        "    loss_gradient = 2*(pred-y[i])\n",
        "    \n",
        "    w0 -= learning_rate * loss_gradient\n",
        "\n",
        "    for index in range(indptr[i], indptr[i + 1]):\n",
        "      feature = indices[index]\n",
        "      w[feature] -= learning_rate * (loss_gradient * data[index] + 2 * reg_w * w[feature])\n",
        "\n",
        "    for factor in range(n_factors):\n",
        "      for index in range(indptr[i], indptr[i + 1]):\n",
        "        feature = indices[index]\n",
        "        term = summed[factor] - v[factor, feature] * data[index]\n",
        "        v_gradient = loss_gradient * data[index] * term\n",
        "        v[factor, feature] -= learning_rate * (v_gradient + 2 * reg_v * v[factor, feature])\n",
        "    \n",
        "  loss /= n_samples\n",
        "  return loss\n",
        "\n",
        "\n",
        "@njit\n",
        "def _predict_instance(data, indptr, indices, w0, w, v, n_factors, i):\n",
        "    \"\"\"predicting a single instance\"\"\"\n",
        "  summed = np.zeros(n_factors)\n",
        "  summed_squared = np.zeros(n_factors)\n",
        "\n",
        "  pred = w0\n",
        "  for index in range(indptr[i], indptr[i + 1]):\n",
        "    feature = indices[index]\n",
        "    pred += w[feature] * data[index]\n",
        "\n",
        "  for factor in range(n_factors):\n",
        "    for index in range(indptr[i], indptr[i + 1]):\n",
        "      feature = indices[index]\n",
        "      term = v[factor, feature] * data[index]\n",
        "      summed[factor] += term\n",
        "      summed_squared[factor] += term * term\n",
        "\n",
        "    pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
        "    \n",
        "  return pred, summed\n",
        "\n",
        "\n",
        "@njit\n",
        "def _log_loss(pred, y):\n",
        "  return (pred-y)**2"
      ],
      "metadata": {
        "id": "0fOCoSjcUjed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}